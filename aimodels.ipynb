{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Considering this name for the package. Doesn't seem to be currently taken in PyPI\n",
    "# import aimodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) \n",
    "\n",
    "# Thinking this and the `Provider Interface should move to the\n",
    "# `framework` directory\n",
    "class ChatCompletionResponse:\n",
    "    class Choice:\n",
    "        class Message:\n",
    "            def __init__(self):\n",
    "                self.content = None\n",
    "\n",
    "        def __init__(self):\n",
    "            self.message = ChatCompletionResponse.Choice.Message()\n",
    "\n",
    "    def __init__(self):\n",
    "        self.choices = [ChatCompletionResponse.Choice()]  # Adjust the range as needed for more choices\n",
    "\n",
    "class ProviderInterface():\n",
    "    def chat_completion_create(self, messages=None, model=None, temperature=0):\n",
    "        raise NotImplementedError(\"Provider Interface has not implemented chat_completion_create()\") \n",
    "\n",
    "# These should go in the `providers` package\n",
    "class OpenAIInterface(ProviderInterface):\n",
    "    def __init__(self):\n",
    "        import openai \n",
    "        self.openai_client = openai.OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "    def chat_completion_create(self, messages=None, model=None, temperature=0):\n",
    "        return self.openai_client.chat.completions.create(model=model, temperature=temperature, messages=messages)\n",
    "    \n",
    "    \n",
    "class AnthropicInterface(ProviderInterface):\n",
    "    def __init__(self):\n",
    "        import anthropic\n",
    "        self.anthropic_client = anthropic.Anthropic(api_key=os.getenv(\"ANTHROPIC_API_KEY\"))\n",
    "\n",
    "    def chat_completion_create(self, messages=None, model=None, temperature=0):\n",
    "        anthropic_messages = []\n",
    "        system_message = None \n",
    "        for msg in messages:\n",
    "            if \"role\" in msg and \"content\" in msg:\n",
    "                if msg[\"role\"] == \"system\":\n",
    "                    system_message = msg[\"content\"]\n",
    "                    #print(f\"Found system message: {system_message}\")\n",
    "                else:\n",
    "                    temp_msg = msg.copy()\n",
    "                    temp_msg[\"content\"] = [{\"type\": \"text\",\"text\": temp_msg[\"content\"] }]\n",
    "                    anthropic_messages.append(temp_msg)\n",
    "        #print(f\"Messages: {messages}\\nAnthropic messages: {anthropic_messages}\")\n",
    "    \n",
    "        if system_message is None:\n",
    "            response = self.anthropic_client.messages.create(messages=anthropic_messages, model=model, max_tokens = 4096, temperature=temperature)\n",
    "        else: \n",
    "            response = self.anthropic_client.messages.create(messages=anthropic_messages, model=model, system=system_message, max_tokens = 4096, temperature=temperature)\n",
    "\n",
    "        text_response = response.content[0].text\n",
    "\n",
    "        chat_completion_response = ChatCompletionResponse()\n",
    "        chat_completion_response.choices[0].message.content = text_response\n",
    "\n",
    "        return chat_completion_response \n",
    "\n",
    "class GroqInterface(ProviderInterface):\n",
    "    def __init__(self):\n",
    "        import groq\n",
    "        self.groq_client = groq.Groq(api_key=os.getenv(\"GROQ_API_KEY\"))\n",
    "\n",
    "    def chat_completion_create(self, messages=None, model=None, temperature=0):\n",
    "        return self.groq_client.chat.completions.create(model=model, temperature=temperature, messages=messages)\n",
    "\n",
    "class AWSInterface(ProviderInterface):\n",
    "    def __init__(self):\n",
    "        import boto3\n",
    "        import json\n",
    "        self.aws_client = boto3.client(service_name=\"bedrock-runtime\", region_name=\"us-east-1\")\n",
    "        self.json = json\n",
    "        \n",
    "    def chat_completion_create(self, messages=None, model=None, temperature=0):\n",
    "        anthropic_messages = []\n",
    "        system_message = None \n",
    "        for msg in messages:\n",
    "            if \"role\" in msg and \"content\" in msg:\n",
    "                if msg[\"role\"] == \"system\":\n",
    "                    system_message = msg[\"content\"]\n",
    "                else:\n",
    "                    temp_msg = msg.copy()\n",
    "                    temp_msg[\"content\"] = [{\"type\": \"text\",\"text\": temp_msg[\"content\"] }]\n",
    "                    anthropic_messages.append(temp_msg)\n",
    "        #print(f\"Messages: {messages}\\nAnthropic messages: {anthropic_messages}\")\n",
    "    \n",
    "        if system_message is None:\n",
    "            response = self.aws_client.invoke_model(\n",
    "                modelId=model,\n",
    "                body=self.json.dumps({\"anthropic_version\": \"bedrock-2023-05-31\", \"max_tokens\": 4096, \"messages\": anthropic_messages}),\n",
    "            )\n",
    "        else: \n",
    "            response = self.aws_client.invoke_model(\n",
    "                modelId=model, \n",
    "                body=self.json.dumps({\"anthropic_version\": \"bedrock-2023-05-31\", \"max_tokens\": 4096, \"system\":system_message, \"messages\": anthropic_messages}),\n",
    "            )\n",
    "        \n",
    "        temp = self.json.loads(response.get(\"body\").read()) \n",
    "        output_list = temp.get(\"content\", [])\n",
    "        temp2 = output_list[0].get(\"text\")\n",
    "\n",
    "        chat_completion_response = ChatCompletionResponse()\n",
    "        chat_completion_response.choices[0].message.content = temp2\n",
    "        return chat_completion_response \n",
    "\n",
    "        #input_tokens = result[\"usage\"][\"input_tokens\"]\n",
    "        #output_tokens = result[\"usage\"][\"output_tokens\"]\n",
    "\n",
    "\n",
    "class OllamaInterface(ProviderInterface):\n",
    "    def __init__(self):\n",
    "        import ollama\n",
    "        self.ollama = ollama\n",
    "\n",
    "    def chat_completion_create(self, messages=None, model=None, temperature=0):\n",
    "        response = self.ollama.chat(model=model, messages=messages)\n",
    "        text_response = response['message']['content']\n",
    "        chat_completion_response = ChatCompletionResponse()\n",
    "        chat_completion_response.choices[0].message.content = text_response\n",
    "\n",
    "        return chat_completion_response \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "Results from anthropic:claude-3-opus-20240229: \n",
      "\n",
      "Arrr, me bucko, 'ere be a jolly jest fer ye!\n",
      "\n",
      "What did th' pirate say on 'is 80th birthday? \"Aye matey!\"\n",
      "\n",
      "Ye see, it be a play on words, as \"Aye matey\" sounds like \"I'm eighty\". Har har har! 'Tis a clever bit o' pirate humor, if I do say so meself. Now, 'ow about ye fetch me a mug o' grog while I spin ye another yarn?\n",
      "----------\n",
      "Results from aws:anthropic.claude-3-sonnet-20240229-v1:0: \n",
      "\n",
      "Ahoy, matey! 'Tis a fine day fer a jolly joke, ain't it? Here be one fer ye:\n",
      "\n",
      "Why did th' pirate buy a trampoline? 'Cause he wanted t' bounce off th' ocean waves! Arr arr arr!\n",
      "----------\n",
      "Results from openai:gpt-4-turbo: \n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 57\u001B[0m\n\u001B[1;32m     55\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m model_id \u001B[38;5;129;01min\u001B[39;00m models_list:\n\u001B[1;32m     56\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m----------\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mResults from \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: \u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 57\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[43mclient\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mchat\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompletions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmessages\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmessages\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     58\u001B[0m     \u001B[38;5;28mprint\u001B[39m(response\u001B[38;5;241m.\u001B[39mchoices[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mmessage\u001B[38;5;241m.\u001B[39mcontent) \n",
      "Cell \u001B[0;32mIn[2], line 45\u001B[0m, in \u001B[0;36mMultiFMClient.Chat.Completions.create\u001B[0;34m(self, model, temperature, messages)\u001B[0m\n\u001B[1;32m     43\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcreate\u001B[39m(\u001B[38;5;28mself\u001B[39m, model\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, temperature\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m, messages\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m     44\u001B[0m     (interface, modelname) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtopmost_instance\u001B[38;5;241m.\u001B[39mget_provider_interface(model)\n\u001B[0;32m---> 45\u001B[0m     temp \u001B[38;5;241m=\u001B[39m \u001B[43minterface\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mchat_completion_create\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodelname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     46\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m temp\n",
      "Cell \u001B[0;32mIn[1], line 28\u001B[0m, in \u001B[0;36mOpenAIInterface.chat_completion_create\u001B[0;34m(self, messages, model, temperature)\u001B[0m\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mchat_completion_create\u001B[39m(\u001B[38;5;28mself\u001B[39m, messages\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, model\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, temperature\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m):\n\u001B[0;32m---> 28\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mopenai_client\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mchat\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompletions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtemperature\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtemperature\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmessages\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmessages\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Dropbox/Code/clean/.venv_python3.10/lib/python3.10/site-packages/openai/_utils/_utils.py:277\u001B[0m, in \u001B[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    275\u001B[0m             msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMissing required argument: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mquote(missing[\u001B[38;5;241m0\u001B[39m])\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    276\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(msg)\n\u001B[0;32m--> 277\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Dropbox/Code/clean/.venv_python3.10/lib/python3.10/site-packages/openai/resources/chat/completions.py:606\u001B[0m, in \u001B[0;36mCompletions.create\u001B[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, parallel_tool_calls, presence_penalty, response_format, seed, stop, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001B[0m\n\u001B[1;32m    573\u001B[0m \u001B[38;5;129m@required_args\u001B[39m([\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmessages\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m], [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmessages\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstream\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[1;32m    574\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcreate\u001B[39m(\n\u001B[1;32m    575\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    604\u001B[0m     timeout: \u001B[38;5;28mfloat\u001B[39m \u001B[38;5;241m|\u001B[39m httpx\u001B[38;5;241m.\u001B[39mTimeout \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m|\u001B[39m NotGiven \u001B[38;5;241m=\u001B[39m NOT_GIVEN,\n\u001B[1;32m    605\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ChatCompletion \u001B[38;5;241m|\u001B[39m Stream[ChatCompletionChunk]:\n\u001B[0;32m--> 606\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_post\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    607\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m/chat/completions\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    608\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbody\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmaybe_transform\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    609\u001B[0m \u001B[43m            \u001B[49m\u001B[43m{\u001B[49m\n\u001B[1;32m    610\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmessages\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    611\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmodel\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    612\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfrequency_penalty\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrequency_penalty\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    613\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfunction_call\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mfunction_call\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    614\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfunctions\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mfunctions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    615\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mlogit_bias\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogit_bias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    616\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mlogprobs\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogprobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    617\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmax_tokens\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    618\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mn\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    619\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mparallel_tool_calls\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mparallel_tool_calls\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    620\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mpresence_penalty\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mpresence_penalty\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    621\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mresponse_format\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mresponse_format\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    622\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mseed\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mseed\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    623\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstop\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    624\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstream\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    625\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstream_options\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    626\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtemperature\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtemperature\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    627\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtool_choice\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtool_choice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    628\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtools\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtools\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    629\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtop_logprobs\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtop_logprobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    630\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtop_p\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtop_p\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    631\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43muser\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43muser\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    632\u001B[0m \u001B[43m            \u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    633\u001B[0m \u001B[43m            \u001B[49m\u001B[43mcompletion_create_params\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mCompletionCreateParams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    634\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    635\u001B[0m \u001B[43m        \u001B[49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmake_request_options\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    636\u001B[0m \u001B[43m            \u001B[49m\u001B[43mextra_headers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextra_headers\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mextra_query\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextra_query\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mextra_body\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextra_body\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\n\u001B[1;32m    637\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    638\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcast_to\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mChatCompletion\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    639\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    640\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mStream\u001B[49m\u001B[43m[\u001B[49m\u001B[43mChatCompletionChunk\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    641\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Dropbox/Code/clean/.venv_python3.10/lib/python3.10/site-packages/openai/_base_client.py:1240\u001B[0m, in \u001B[0;36mSyncAPIClient.post\u001B[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001B[0m\n\u001B[1;32m   1226\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpost\u001B[39m(\n\u001B[1;32m   1227\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   1228\u001B[0m     path: \u001B[38;5;28mstr\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1235\u001B[0m     stream_cls: \u001B[38;5;28mtype\u001B[39m[_StreamT] \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   1236\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ResponseT \u001B[38;5;241m|\u001B[39m _StreamT:\n\u001B[1;32m   1237\u001B[0m     opts \u001B[38;5;241m=\u001B[39m FinalRequestOptions\u001B[38;5;241m.\u001B[39mconstruct(\n\u001B[1;32m   1238\u001B[0m         method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpost\u001B[39m\u001B[38;5;124m\"\u001B[39m, url\u001B[38;5;241m=\u001B[39mpath, json_data\u001B[38;5;241m=\u001B[39mbody, files\u001B[38;5;241m=\u001B[39mto_httpx_files(files), \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions\n\u001B[1;32m   1239\u001B[0m     )\n\u001B[0;32m-> 1240\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(ResponseT, \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcast_to\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mopts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream_cls\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[0;32m~/Dropbox/Code/clean/.venv_python3.10/lib/python3.10/site-packages/openai/_base_client.py:921\u001B[0m, in \u001B[0;36mSyncAPIClient.request\u001B[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001B[0m\n\u001B[1;32m    912\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrequest\u001B[39m(\n\u001B[1;32m    913\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    914\u001B[0m     cast_to: Type[ResponseT],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    919\u001B[0m     stream_cls: \u001B[38;5;28mtype\u001B[39m[_StreamT] \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    920\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ResponseT \u001B[38;5;241m|\u001B[39m _StreamT:\n\u001B[0;32m--> 921\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    922\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcast_to\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcast_to\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    923\u001B[0m \u001B[43m        \u001B[49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    924\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    925\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream_cls\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    926\u001B[0m \u001B[43m        \u001B[49m\u001B[43mremaining_retries\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mremaining_retries\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    927\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Dropbox/Code/clean/.venv_python3.10/lib/python3.10/site-packages/openai/_base_client.py:952\u001B[0m, in \u001B[0;36mSyncAPIClient._request\u001B[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001B[0m\n\u001B[1;32m    949\u001B[0m log\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSending HTTP Request: \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, request\u001B[38;5;241m.\u001B[39mmethod, request\u001B[38;5;241m.\u001B[39murl)\n\u001B[1;32m    951\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 952\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_client\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    953\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    954\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_should_stream_response_body\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    955\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    956\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    957\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m httpx\u001B[38;5;241m.\u001B[39mTimeoutException \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[1;32m    958\u001B[0m     log\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEncountered httpx.TimeoutException\u001B[39m\u001B[38;5;124m\"\u001B[39m, exc_info\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[0;32m~/Dropbox/Code/clean/.venv_python3.10/lib/python3.10/site-packages/httpx/_client.py:914\u001B[0m, in \u001B[0;36mClient.send\u001B[0;34m(self, request, stream, auth, follow_redirects)\u001B[0m\n\u001B[1;32m    906\u001B[0m follow_redirects \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    907\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfollow_redirects\n\u001B[1;32m    908\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(follow_redirects, UseClientDefault)\n\u001B[1;32m    909\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m follow_redirects\n\u001B[1;32m    910\u001B[0m )\n\u001B[1;32m    912\u001B[0m auth \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_request_auth(request, auth)\n\u001B[0;32m--> 914\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_send_handling_auth\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    915\u001B[0m \u001B[43m    \u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    916\u001B[0m \u001B[43m    \u001B[49m\u001B[43mauth\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mauth\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    917\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfollow_redirects\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfollow_redirects\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    918\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhistory\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    919\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    920\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    921\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m stream:\n",
      "File \u001B[0;32m~/Dropbox/Code/clean/.venv_python3.10/lib/python3.10/site-packages/httpx/_client.py:942\u001B[0m, in \u001B[0;36mClient._send_handling_auth\u001B[0;34m(self, request, auth, follow_redirects, history)\u001B[0m\n\u001B[1;32m    939\u001B[0m request \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mnext\u001B[39m(auth_flow)\n\u001B[1;32m    941\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m--> 942\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_send_handling_redirects\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    943\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    944\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfollow_redirects\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfollow_redirects\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    945\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhistory\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhistory\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    946\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    947\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    948\u001B[0m         \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[0;32m~/Dropbox/Code/clean/.venv_python3.10/lib/python3.10/site-packages/httpx/_client.py:979\u001B[0m, in \u001B[0;36mClient._send_handling_redirects\u001B[0;34m(self, request, follow_redirects, history)\u001B[0m\n\u001B[1;32m    976\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m hook \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_event_hooks[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrequest\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n\u001B[1;32m    977\u001B[0m     hook(request)\n\u001B[0;32m--> 979\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_send_single_request\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    980\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    981\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m hook \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_event_hooks[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mresponse\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n",
      "File \u001B[0;32m~/Dropbox/Code/clean/.venv_python3.10/lib/python3.10/site-packages/httpx/_client.py:1015\u001B[0m, in \u001B[0;36mClient._send_single_request\u001B[0;34m(self, request)\u001B[0m\n\u001B[1;32m   1010\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m   1011\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAttempted to send an async request with a sync Client instance.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1012\u001B[0m     )\n\u001B[1;32m   1014\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m request_context(request\u001B[38;5;241m=\u001B[39mrequest):\n\u001B[0;32m-> 1015\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[43mtransport\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhandle_request\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1017\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response\u001B[38;5;241m.\u001B[39mstream, SyncByteStream)\n\u001B[1;32m   1019\u001B[0m response\u001B[38;5;241m.\u001B[39mrequest \u001B[38;5;241m=\u001B[39m request\n",
      "File \u001B[0;32m~/Dropbox/Code/clean/.venv_python3.10/lib/python3.10/site-packages/httpx/_transports/default.py:233\u001B[0m, in \u001B[0;36mHTTPTransport.handle_request\u001B[0;34m(self, request)\u001B[0m\n\u001B[1;32m    220\u001B[0m req \u001B[38;5;241m=\u001B[39m httpcore\u001B[38;5;241m.\u001B[39mRequest(\n\u001B[1;32m    221\u001B[0m     method\u001B[38;5;241m=\u001B[39mrequest\u001B[38;5;241m.\u001B[39mmethod,\n\u001B[1;32m    222\u001B[0m     url\u001B[38;5;241m=\u001B[39mhttpcore\u001B[38;5;241m.\u001B[39mURL(\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    230\u001B[0m     extensions\u001B[38;5;241m=\u001B[39mrequest\u001B[38;5;241m.\u001B[39mextensions,\n\u001B[1;32m    231\u001B[0m )\n\u001B[1;32m    232\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m map_httpcore_exceptions():\n\u001B[0;32m--> 233\u001B[0m     resp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_pool\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhandle_request\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreq\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(resp\u001B[38;5;241m.\u001B[39mstream, typing\u001B[38;5;241m.\u001B[39mIterable)\n\u001B[1;32m    237\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m Response(\n\u001B[1;32m    238\u001B[0m     status_code\u001B[38;5;241m=\u001B[39mresp\u001B[38;5;241m.\u001B[39mstatus,\n\u001B[1;32m    239\u001B[0m     headers\u001B[38;5;241m=\u001B[39mresp\u001B[38;5;241m.\u001B[39mheaders,\n\u001B[1;32m    240\u001B[0m     stream\u001B[38;5;241m=\u001B[39mResponseStream(resp\u001B[38;5;241m.\u001B[39mstream),\n\u001B[1;32m    241\u001B[0m     extensions\u001B[38;5;241m=\u001B[39mresp\u001B[38;5;241m.\u001B[39mextensions,\n\u001B[1;32m    242\u001B[0m )\n",
      "File \u001B[0;32m~/Dropbox/Code/clean/.venv_python3.10/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:216\u001B[0m, in \u001B[0;36mConnectionPool.handle_request\u001B[0;34m(self, request)\u001B[0m\n\u001B[1;32m    213\u001B[0m         closing \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_assign_requests_to_connections()\n\u001B[1;32m    215\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_close_connections(closing)\n\u001B[0;32m--> 216\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m exc \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    218\u001B[0m \u001B[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001B[39;00m\n\u001B[1;32m    219\u001B[0m \u001B[38;5;66;03m# the point at which the response is closed.\u001B[39;00m\n\u001B[1;32m    220\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response\u001B[38;5;241m.\u001B[39mstream, Iterable)\n",
      "File \u001B[0;32m~/Dropbox/Code/clean/.venv_python3.10/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:196\u001B[0m, in \u001B[0;36mConnectionPool.handle_request\u001B[0;34m(self, request)\u001B[0m\n\u001B[1;32m    192\u001B[0m connection \u001B[38;5;241m=\u001B[39m pool_request\u001B[38;5;241m.\u001B[39mwait_for_connection(timeout\u001B[38;5;241m=\u001B[39mtimeout)\n\u001B[1;32m    194\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    195\u001B[0m     \u001B[38;5;66;03m# Send the request on the assigned connection.\u001B[39;00m\n\u001B[0;32m--> 196\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[43mconnection\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhandle_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    197\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpool_request\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\n\u001B[1;32m    198\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    199\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m ConnectionNotAvailable:\n\u001B[1;32m    200\u001B[0m     \u001B[38;5;66;03m# In some cases a connection may initially be available to\u001B[39;00m\n\u001B[1;32m    201\u001B[0m     \u001B[38;5;66;03m# handle a request, but then become unavailable.\u001B[39;00m\n\u001B[1;32m    202\u001B[0m     \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[1;32m    203\u001B[0m     \u001B[38;5;66;03m# In this case we clear the connection and try again.\u001B[39;00m\n\u001B[1;32m    204\u001B[0m     pool_request\u001B[38;5;241m.\u001B[39mclear_connection()\n",
      "File \u001B[0;32m~/Dropbox/Code/clean/.venv_python3.10/lib/python3.10/site-packages/httpcore/_sync/connection.py:101\u001B[0m, in \u001B[0;36mHTTPConnection.handle_request\u001B[0;34m(self, request)\u001B[0m\n\u001B[1;32m     98\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_connect_failed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m     99\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m exc\n\u001B[0;32m--> 101\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_connection\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhandle_request\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Dropbox/Code/clean/.venv_python3.10/lib/python3.10/site-packages/httpcore/_sync/http11.py:143\u001B[0m, in \u001B[0;36mHTTP11Connection.handle_request\u001B[0;34m(self, request)\u001B[0m\n\u001B[1;32m    141\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m Trace(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mresponse_closed\u001B[39m\u001B[38;5;124m\"\u001B[39m, logger, request) \u001B[38;5;28;01mas\u001B[39;00m trace:\n\u001B[1;32m    142\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_response_closed()\n\u001B[0;32m--> 143\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m exc\n",
      "File \u001B[0;32m~/Dropbox/Code/clean/.venv_python3.10/lib/python3.10/site-packages/httpcore/_sync/http11.py:113\u001B[0m, in \u001B[0;36mHTTP11Connection.handle_request\u001B[0;34m(self, request)\u001B[0m\n\u001B[1;32m    102\u001B[0m     \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[1;32m    104\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Trace(\n\u001B[1;32m    105\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mreceive_response_headers\u001B[39m\u001B[38;5;124m\"\u001B[39m, logger, request, kwargs\n\u001B[1;32m    106\u001B[0m ) \u001B[38;5;28;01mas\u001B[39;00m trace:\n\u001B[1;32m    107\u001B[0m     (\n\u001B[1;32m    108\u001B[0m         http_version,\n\u001B[1;32m    109\u001B[0m         status,\n\u001B[1;32m    110\u001B[0m         reason_phrase,\n\u001B[1;32m    111\u001B[0m         headers,\n\u001B[1;32m    112\u001B[0m         trailing_data,\n\u001B[0;32m--> 113\u001B[0m     ) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_receive_response_headers\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    114\u001B[0m     trace\u001B[38;5;241m.\u001B[39mreturn_value \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    115\u001B[0m         http_version,\n\u001B[1;32m    116\u001B[0m         status,\n\u001B[1;32m    117\u001B[0m         reason_phrase,\n\u001B[1;32m    118\u001B[0m         headers,\n\u001B[1;32m    119\u001B[0m     )\n\u001B[1;32m    121\u001B[0m network_stream \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_network_stream\n",
      "File \u001B[0;32m~/Dropbox/Code/clean/.venv_python3.10/lib/python3.10/site-packages/httpcore/_sync/http11.py:186\u001B[0m, in \u001B[0;36mHTTP11Connection._receive_response_headers\u001B[0;34m(self, request)\u001B[0m\n\u001B[1;32m    183\u001B[0m timeout \u001B[38;5;241m=\u001B[39m timeouts\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mread\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[1;32m    185\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m--> 186\u001B[0m     event \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_receive_event\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    187\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(event, h11\u001B[38;5;241m.\u001B[39mResponse):\n\u001B[1;32m    188\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[0;32m~/Dropbox/Code/clean/.venv_python3.10/lib/python3.10/site-packages/httpcore/_sync/http11.py:224\u001B[0m, in \u001B[0;36mHTTP11Connection._receive_event\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    221\u001B[0m     event \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_h11_state\u001B[38;5;241m.\u001B[39mnext_event()\n\u001B[1;32m    223\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m event \u001B[38;5;129;01mis\u001B[39;00m h11\u001B[38;5;241m.\u001B[39mNEED_DATA:\n\u001B[0;32m--> 224\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_network_stream\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    225\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mREAD_NUM_BYTES\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\n\u001B[1;32m    226\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    228\u001B[0m     \u001B[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001B[39;00m\n\u001B[1;32m    229\u001B[0m     \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[1;32m    230\u001B[0m     \u001B[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    234\u001B[0m     \u001B[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001B[39;00m\n\u001B[1;32m    235\u001B[0m     \u001B[38;5;66;03m# it as a ConnectError.\u001B[39;00m\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m data \u001B[38;5;241m==\u001B[39m \u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_h11_state\u001B[38;5;241m.\u001B[39mtheir_state \u001B[38;5;241m==\u001B[39m h11\u001B[38;5;241m.\u001B[39mSEND_RESPONSE:\n",
      "File \u001B[0;32m~/Dropbox/Code/clean/.venv_python3.10/lib/python3.10/site-packages/httpcore/_backends/sync.py:126\u001B[0m, in \u001B[0;36mSyncStream.read\u001B[0;34m(self, max_bytes, timeout)\u001B[0m\n\u001B[1;32m    124\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m map_exceptions(exc_map):\n\u001B[1;32m    125\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sock\u001B[38;5;241m.\u001B[39msettimeout(timeout)\n\u001B[0;32m--> 126\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sock\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrecv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmax_bytes\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/homebrew/Cellar/python@3.10/3.10.13_2/Frameworks/Python.framework/Versions/3.10/lib/python3.10/ssl.py:1292\u001B[0m, in \u001B[0;36mSSLSocket.recv\u001B[0;34m(self, buflen, flags)\u001B[0m\n\u001B[1;32m   1288\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m flags \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m   1289\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   1290\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnon-zero flags not allowed in calls to recv() on \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m\n\u001B[1;32m   1291\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m)\n\u001B[0;32m-> 1292\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbuflen\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1293\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1294\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mrecv(buflen, flags)\n",
      "File \u001B[0;32m/opt/homebrew/Cellar/python@3.10/3.10.13_2/Frameworks/Python.framework/Versions/3.10/lib/python3.10/ssl.py:1165\u001B[0m, in \u001B[0;36mSSLSocket.read\u001B[0;34m(self, len, buffer)\u001B[0m\n\u001B[1;32m   1163\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sslobj\u001B[38;5;241m.\u001B[39mread(\u001B[38;5;28mlen\u001B[39m, buffer)\n\u001B[1;32m   1164\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1165\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sslobj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1166\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m SSLError \u001B[38;5;28;01mas\u001B[39;00m x:\n\u001B[1;32m   1167\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m x\u001B[38;5;241m.\u001B[39margs[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m==\u001B[39m SSL_ERROR_EOF \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msuppress_ragged_eofs:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# should we make a `client` package for these?\n",
    "class MultiFMClient:\n",
    "    def __init__(self):\n",
    "        self.chat = self.Chat(self)\n",
    "        self.num=10\n",
    "        self.all_interfaces = {} \n",
    "        self.all_factories = {\"openai\":OpenAIInterface, \n",
    "                              \"groq\":GroqInterface,\n",
    "                              \"anthropic\":AnthropicInterface,\n",
    "                              \"aws\":AWSInterface,\n",
    "                              \"ollama\":OllamaInterface} \n",
    "\n",
    "    def get_provider_interface(self, model):\n",
    "        # Given a model name like \"openai:gpt-4\" split it into provider=\"openai\" and model_name=\"gpt-4\" \n",
    "        # We split on the first :, so \"ollama:llama3:8b\" gets split into \"ollama\" and \"llama3:8b\" \n",
    "        if not \":\" in model:\n",
    "            raise ValueError(f\"Expected : in model identifier to specify provider:model. Got {model}\")\n",
    "        model_parts = model.split(\":\", maxsplit=1)\n",
    "        provider = model_parts[0]\n",
    "        model_name = model_parts[1]\n",
    "        # print(f\"Provider: {provider}, modelname: {model_name}\")\n",
    "        \n",
    "        if provider in self.all_interfaces:\n",
    "            #print(f(\"Found provider {provider} in all_interfaces\")) \n",
    "            return self.all_interfaces[provider] \n",
    "\n",
    "        if not provider in self.all_factories:\n",
    "            raise Exception(f\"Could not find factory to create interface for provider {provider}\") \n",
    "\n",
    "        self.all_interfaces[provider] = self.all_factories[provider]() \n",
    "\n",
    "        return (self.all_interfaces[provider], model_name)\n",
    "    \n",
    "    class Chat:\n",
    "        def __init__(self, topmost_instance):\n",
    "            self.topmost_instance = topmost_instance \n",
    "            self.completions = self.Completions(topmost_instance)\n",
    "\n",
    "\n",
    "        class Completions:\n",
    "            def __init__(self, topmost_instance):\n",
    "                self.topmost_instance = topmost_instance \n",
    "\n",
    "            def create(self, model=None, temperature=0, messages=None):\n",
    "                (interface, modelname) = self.topmost_instance.get_provider_interface(model)\n",
    "                temp = interface.chat_completion_create(messages, model=modelname)\n",
    "                return temp\n",
    "\n",
    "# Usage example\n",
    "client = MultiFMClient()\n",
    "messages=[{\"role\": \"system\", \"content\": \"Respond in Pirate English.\"}, \n",
    "          {\"role\": \"user\", \"content\": \"Tell me a joke\"} ] \n",
    "\n",
    "models_list = [\"anthropic:claude-3-opus-20240229\", \"aws:anthropic.claude-3-sonnet-20240229-v1:0\", \"openai:gpt-4-turbo\", \"groq:mixtral-8x7b-32768\", ] \n",
    "\n",
    "for model_id in models_list:\n",
    "    print(f\"----------\\nResults from {model_id}: \\n\")\n",
    "    response = client.chat.completions.create(model=model_id, messages=messages)\n",
    "    print(response.choices[0].message.content) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiFM:\n",
    "    def __init__(self):\n",
    "        self.temp = 5\n",
    "    def set_temp(self, newtemp):\n",
    "        self.temp = newtemp\n",
    "    def returntemp(self):\n",
    "        return self.temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Older version "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "import os \n",
    "from groq import Groq\n",
    "import anthropic\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) \n",
    "\n",
    "openai_client = openai.OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "anthropic_client = anthropic.Anthropic(api_key=os.environ.get(\"ANTHROPIC_API_KEY\"))\n",
    "groq_client = Groq(api_key=os.environ.get(\"GROQ_API_KEY\"))\n",
    "\n",
    "\n",
    "def get_response_to_prompt(prompt,model=\"openai:gpt-4-turbo\"):\n",
    "    model_parts = model.split(\":\")\n",
    "    fm_provider = model_parts[0]\n",
    "    model = model_parts[1]\n",
    "\n",
    "    match fm_provider:\n",
    "        case \"openai\":\n",
    "            response = openai_client.chat.completions.create(model=model, temperature=0, messages=[\n",
    "                # {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt} ] )\n",
    "            return response.choices[0].message.content \n",
    "\n",
    "        case \"anthropic\":\n",
    "            response = anthropic_client.messages.create(model=model, max_tokens=1000, temperature=0,\n",
    "                messages=[{\"role\": \"user\",\"content\": [\n",
    "                    {\"type\": \"text\",\"text\": prompt }] }] )\n",
    "            return response.content[0].text\n",
    "\n",
    "        case \"groq\": \n",
    "            response = groq_client.chat.completions.create(\n",
    "                messages=[{\"role\": \"user\",\"content\": prompt}], model=model)\n",
    "            return response.choices[0].message.content\n",
    "        case _:\n",
    "            raise Exception(f\"FM Provider {provider} not found\")\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The gravity on Jupiter is significantly stronger than on Earth. On Jupiter, the gravitational force is about 24.79 m/s, which is approximately 2.5 times stronger than the gravitational force on Earth, which is about 9.8 m/s. This means that if you could stand on Jupiter (which is impossible due to its lack of a solid surface and extreme atmospheric conditions), you would feel much heavier.\n",
      "The gravity on Jupiter varies depending on the distance from the center of the planet, as Jupiter is a gas giant and does not have a solid surface like Earth. However, we can calculate the gravity at the \"surface\" of Jupiter, which is typically defined as the point where the atmospheric pressure is equal to 1 bar (similar to Earth's atmospheric pressure at sea level).\n",
      "\n",
      "At Jupiter's \"surface,\" the acceleration due to gravity is approximately 24.79 m/s (meters per second squared). This is about 2.53 times the gravity on Earth's surface, which is 9.81 m/s.\n",
      "\n",
      "To put this into perspective, if you weighed 150 pounds (68 kg) on Earth, you would weigh about 379.5 pounds (172 kg) on Jupiter's \"surface.\" However, since Jupiter is a gas giant, you would not be able to stand on its surface as you would sink into its atmosphere.\n",
      "The surface gravity on Jupiter is approximately 2.53 times that of Earth. This is because Jupiter is a massive planet, with a diameter about 11 times larger than Earth's. However, it's important to note that Jupiter doesn't have a solid surface like Earth, so we can't stand on it. The value I gave is an approximation based on the gravitational force at the \"surface\" of Jupiter, which would be at the boundary between its atmosphere and the denser inner layers.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What's the gravity on Jupiter?\"\n",
    "\n",
    "temp1 = get_response_to_prompt(prompt,model=\"openai:gpt-4-turbo\")\n",
    "print(temp1)\n",
    "temp2 = get_response_to_prompt(prompt,model=\"anthropic:claude-3-opus-20240229\")\n",
    "\n",
    "print(temp2)\n",
    "temp3 = get_response_to_prompt(prompt,model=\"groq:mixtral-8x7b-32768\")\n",
    "print(temp3) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The gravity on Jupiter varies depending on the distance from the center of the planet, as Jupiter is a gas giant and does not have a solid surface like Earth. However, we can calculate the gravity at the \"surface\" of Jupiter, which is typically defined as the point where the atmospheric pressure is equal to 1 bar (similar to Earth's atmospheric pressure at sea level).\n",
      "\n",
      "At Jupiter's \"surface,\" the acceleration due to gravity is approximately 24.79 m/s (meters per second squared). This is about 2.53 times the gravity on Earth's surface, which is 9.81 m/s.\n",
      "\n",
      "To put this into perspective, if you weighed 150 pounds (68 kg) on Earth, you would weigh about 379.5 pounds (172 kg) on Jupiter's \"surface.\" However, since Jupiter is a gas giant, you would not be able to stand on its surface as you would sink into its atmosphere.\n"
     ]
    }
   ],
   "source": [
    "print(temp2[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The surface gravity of Jupiter is approximately 24.79 meters per second squared (m/s), which is about 2.5 times the surface gravity of Earth, which is about 9.8 m/s.\n",
      "\n",
      "This means that if you weighed 100 pounds (45 kg) on Earth, you would weigh around 250 pounds (113 kg) on Jupiter!\n",
      "\n",
      "It's worth noting that Jupiter is a gas giant planet, so it doesn't really have a solid surface. The gravity measurement I mentioned is based on the gravitational acceleration at a specific altitude, called the \"1-bar level,\" which is defined as the point where the atmospheric pressure is equal to 1 bar (about 1 atmosphere).\n"
     ]
    }
   ],
   "source": [
    "print(get_response_to_prompt(prompt,model=\"groq:llama3-70b-8192\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def raw_response_to_prompt(prompt,model=\"gpt-4-turbo\"):\n",
    "    return client.chat.completions.create(model=model, temperature=0, messages=[\n",
    "            # {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POSITIVE\n"
     ]
    }
   ],
   "source": [
    "response = get_response_to_prompt(\"\"\"\n",
    "Classify the following as having either a POSITIVE or NEGATIVE sentiment:\n",
    "\n",
    "---\n",
    "The HP meeting was great! Lots of exciting progress and discussios on AI. \n",
    "---\n",
    "\"\"\")\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-9JtlARfwqtAEoxpnp65EwOpjO7HVe', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='POSITIVE', role='assistant', function_call=None, tool_calls=None))], created=1714529600, model='gpt-4-turbo-2024-04-09', object='chat.completion', system_fingerprint='fp_46a93fa712', usage=CompletionUsage(completion_tokens=2, prompt_tokens=42, total_tokens=44))\n"
     ]
    }
   ],
   "source": [
    "response = raw_response_to_prompt(\"\"\"\n",
    "Classify the following as having either a POSITIVE or NEGATIVE sentiment:\n",
    "\n",
    "---\n",
    "The HP meeting was great! Lots of exciting progress and discussios on AI. \n",
    "---\n",
    "\"\"\")\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "openai.types.chat.chat_completion.ChatCompletion"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'error' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[58], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43merror\u001B[49m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFdsF\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'error' is not defined"
     ]
    }
   ],
   "source": [
    "error(\"FdsF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ContentBlock(text=\"The gravity on Mars is approximately 3.71 m/s, which is about 38% of the gravity on Earth (9.81 m/s). This means that an object on Mars would weigh about 38% of its weight on Earth.\\n\\nThe lower gravity on Mars is due to its smaller mass compared to Earth. Mars has a mass of about 6.39  10^23 kg, which is roughly 10.7% of Earth's mass (5.97  10^24 kg).\\n\\nThe lower gravity on Mars has implications for future human exploration and potential colonization. Astronauts on Mars would experience reduced physical stress on their bodies, but they would also need to adapt to the differences in gravity for daily activities and movement. The lower gravity also affects the planet's ability to retain atmosphere, which is one of the reasons why Mars has a much thinner atmosphere compared to Earth.\", type='text')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "client = anthropic.Anthropic(\n",
    "    # defaults to os.environ.get(\"ANTHROPIC_API_KEY\")\n",
    "    api_key= os.environ.get(\"ANTHROPIC_API_KEY\"),\n",
    ")\n",
    "message = client.messages.create(\n",
    "    model=\"claude-3-opus-20240229\",\n",
    "    max_tokens=1000,\n",
    "    temperature=0,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is gravity on Mars? \"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "print(message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The gravity on Mars is approximately 3.71 m/s, which is about 38% of the gravity on Earth (9.81 m/s). This means that an object on Mars would weigh about 38% of its weight on Earth.\n",
      "\n",
      "The lower gravity on Mars is due to its smaller mass compared to Earth. Mars has a mass of about 6.39  10^23 kg, which is roughly 10.7% of Earth's mass (5.97  10^24 kg).\n",
      "\n",
      "The lower gravity on Mars has implications for future human exploration and potential colonization. Astronauts on Mars would experience reduced physical stress on their bodies, but they would also need to adapt to the differences in gravity for tasks like walking, running, and lifting objects. The lower gravity also affects the planet's ability to retain atmosphere, which is one of the reasons why Mars has a much thinner atmosphere compared to Earth.\n"
     ]
    }
   ],
   "source": [
    "print(message.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "client = myclient.client()\n",
    "client.chat.completions.create(17)\n",
    "\n",
    "\n",
    "\n",
    "class FMClient:\n",
    "    \n",
    "    model_parts = model.split(\":\")\n",
    "    fm_provider = model_parts[0]\n",
    "    model = model_parts[1] \n",
    "\n",
    "    match fm_provider:\n",
    "        case \"openai\":\n",
    "            response = openai_client.chat.completions.create(model=model, temperature=0, messages=[\n",
    "                # {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt} ] )\n",
    "            return response.choices[0].message.content \n",
    "\n",
    "        case \"anthropic\":\n",
    "            response = anthropic_client.messages.create(model=model, max_tokens=1000, temperature=0,\n",
    "                messages=[{\"role\": \"user\",\"content\": [\n",
    "                    {\"type\": \"text\",\"text\": prompt }] }] )\n",
    "            return response.content[0].text\n",
    "\n",
    "        case \"groq\": \n",
    "            response = groq_client.chat.completions.create(\n",
    "                messages=[{\"role\": \"user\",\"content\": prompt}], model=model)\n",
    "            return response.choices[0].message.content\n",
    "        case _:\n",
    "            raise Exception(f\"FM Provider {provider} not found\")\n",
    "\n",
    "            \n",
    "client = openai.OpenAI()\n",
    "\n",
    "\n",
    "def get_response_to_prompt(prompt,model=\"gpt-4-turbo\"):\n",
    "    return client.chat.completions.create(model=model, temperature=0, messages=[\n",
    "            # {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    ).choices[0].message.content\n",
    "response = get_response_to_prompt(\"Tell me a joke.\")\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anthropic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "\n",
    "client = anthropic.Anthropic(\n",
    "    # defaults to os.environ.get(\"ANTHROPIC_API_KEY\")\n",
    "    api_key= os.environ.get(\"ANTHROPIC_API_KEY\"),\n",
    ")\n",
    "message = client.messages.create(\n",
    "    model=\"claude-3-opus-20240229\",\n",
    "    max_tokens=1000,\n",
    "    temperature=0,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"What is gravity on Mars? \"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "print(message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Groq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from groq import Groq\n",
    "\n",
    "client = Groq(api_key=os.environ.get(\"GROQ_API_KEY\"),)\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Tell me a joke about cats\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"mixtral-8x7b-32768\",\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What do you call a pile of kittens? A meow-ntain! I'll show myself out...\n"
     ]
    }
   ],
   "source": [
    "print(chat_completion.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import Ollama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama = Ollama(base_url='http://localhost:11434',model=\"llama2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's one:\n",
      "\n",
      "Why don't scientists trust atoms?\n",
      "\n",
      "Because they make up everything!\n",
      "\n",
      "Hope that made you smile! Do you want to hear another?\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "response = ollama.chat(model='llama3:8b', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'Tell me a joke.',\n",
    "  },\n",
    "])\n",
    "print(response['message']['content'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nSure, here's one:\\n\\nWhy don't scientists trust atoms?\\nBecause they make up everything!\\n\\nI hope you found that amusing!\""
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'llama2', 'created_at': '2024-05-03T00:15:15.95371Z', 'message': {'role': 'assistant', 'content': \"\\nSure, here's one:\\n\\nWhy don't scientists trust atoms?\\nBecause they make up everything!\\n\\nI hope you found that amusing!\"}, 'done': True, 'total_duration': 787894500, 'load_duration': 2937084, 'prompt_eval_duration': 200606000, 'eval_count': 39, 'eval_duration': 581242000}\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# PaLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as palm\n",
    "from google.api_core import client_options as client_options_lib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palm.configure(\n",
    "    api_key=GOOGLE_API_KEY,\n",
    "    transport=\"rest\",\n",
    "    client_options=client_options_lib.ClientOptions(\n",
    "        api_endpoint=os.getenv(\"GOOGLE_API_BASE\"),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [m for m in palm.list_models() \n",
    "          if 'generateText' \n",
    "          in m.supported_generation_methods]\n",
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bison = models[3]\n",
    "model_bison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.api_core import retry\n",
    "@retry.Retry()\n",
    "def generate_text(prompt,\n",
    "                  model=model_bison,\n",
    "                  temperature=0.0):\n",
    "    return palm.generate_text(prompt=prompt,\n",
    "                              model=model,\n",
    "                              temperature=temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"You are an expert in social services, poverty, abuse, and disadvantaged populations.  \n",
    "Brainstorm are some ways to find users of illicit substances such as opioids (ages 18+) in the US so that a \\\n",
    "non-profit can provide services to them.\"\"\"\n",
    "\n",
    "completion = generate_text(prompt)\n",
    "Markdown(completion.result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Markdown(completion.result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import AzureOpenAI\n",
    "from IPython.display import Markdown, display \n",
    "\n",
    "client = AzureOpenAI(\n",
    "  azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\"), \n",
    "  api_key=os.getenv(\"AZURE_OPENAI_KEY\"),  \n",
    "  api_version=\"2023-05-15\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.getenv(\"AZURE_OPENAI_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"Deployment2\", # model = \"deployment_name\".\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Does Azure OpenAI support customer managed keys?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Yes, customer managed keys are supported by Azure OpenAI.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Do other Azure AI services support this too?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#openai.api_key = os.getenv(\"AZURE_OPENAI_KEY\")\n",
    "#openai.api_base = os.getenv(\"AZURE_OPENAI_ENDPOINT\") # your endpoint should look like the following https://YOUR_RESOURCE_NAME.openai.azure.com/\n",
    "#openai.api_type = 'azure'\n",
    "#openai.api_version = '2023-05-15' # this might change in the future\n",
    "\n",
    "#deployment_name='Deployment1' #This will correspond to the custom name you chose for your deployment when you deployed a model. \n",
    "\n",
    "client=openai.OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response_to_prompt(prompt,model=\"gpt-4-1106-preview\"):\n",
    "    return client.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=0, \n",
    "        messages=[\n",
    "            # {\"role\": \"system\", \"content\": \"You are a poetic assistant, skilled in explaining complex programming concepts with creative flair.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "#def get_response_to_prompt(prompt):\n",
    "#    response = openai.ChatCompletion.create(model=\"gpt-4\", \\\n",
    "#               messages=[{\"role\":\"user\",\"content\":prompt}], temperature=0)\n",
    "#    return response.choices[0].message[\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = get_response_to_prompt(\"How to print Hello World\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Markdown(completion.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_response_to_prompt(\"Reverse the word lollipop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send a completion call to generate an answer\n",
    "print('Sending a test completion job')\n",
    "start_phrase = 'Write a tagline for an ice cream shop. '\n",
    "response = openai.Completion.create(engine=deployment_name, prompt=start_phrase, max_tokens=10)\n",
    "text = response['choices'][0]['text'].replace('\\n', '').replace(' .', '.').strip()\n",
    "print(start_phrase+text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS Bedrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock = boto3.client(service_name='bedrock-runtime',region_name='us-west-2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "body = json.dumps({\n",
    "    \"prompt\": \"\\n\\nHuman: explain black holes to 8th graders\\n\\nAssistant:\",\n",
    "    \"max_tokens_to_sample\": 300,\n",
    "    \"temperature\": 0.1,\n",
    "    \"top_p\": 0.9,\n",
    "})\n",
    "\n",
    "modelId = 'anthropic.claude-v2'\n",
    "#modelId = 'anthropic.claude-3-sonnet-20240229-v1:0'\n",
    "accept = 'application/json'\n",
    "contentType = 'application/json'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = bedrock.invoke_model(body=body, modelId=modelId, accept=accept, contentType=contentType)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ResponseMetadata': {'RequestId': '9a880264-0ac8-42ef-8222-2e7377b4dc6b', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Fri, 03 May 2024 05:23:50 GMT', 'content-type': 'application/json', 'content-length': '1445', 'connection': 'keep-alive', 'x-amzn-requestid': '9a880264-0ac8-42ef-8222-2e7377b4dc6b', 'x-amzn-bedrock-invocation-latency': '9970', 'x-amzn-bedrock-output-token-count': '300', 'x-amzn-bedrock-input-token-count': '17'}, 'RetryAttempts': 0}, 'contentType': 'application/json', 'body': <botocore.response.StreamingBody object at 0x122627700>}\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = response['body'].read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'{\"completion\":\" Here is a simple explanation of black holes for 8th graders:\\\\n\\\\n- A black hole is a region in space where gravity is so strong that nothing can escape from it, even light. This region is called the \\\\\"event horizon.\\\\\"\\\\n\\\\n- Black holes form when very massive stars collapse. When the star runs out of nuclear fuel, gravity makes it collapse. The matter gets compressed into a very small space, creating a black hole. \\\\n\\\\n- Black holes are called \\\\\"black\\\\\" because they do not let any light escape. The gravity is so intense that it swallows everything, even light waves. So we can\\'t see black holes directly. We can only detect them from their effects on nearby stars and gas.\\\\n\\\\n- Black holes have such strong gravity because all their mass is concentrated in a very small space. This creates a very steep gravitational slope near the event horizon that traps everything. \\\\n\\\\n- Not even light can escape once it crosses the event horizon. That\\'s why it\\'s called the \\\\\"point of no return.\\\\\" Anything that goes beyond the event horizon is lost forever to the black hole.\\\\n\\\\n- Black holes do not \\\\\"suck\\\\\" matter in. They simply have gravity like everything else. But their gravity is so extreme that nothing can escape once it gets close.\\\\n\\\\n- We can detect black holes from the X-rays and other radiation given off by material as it swirls around the black hole before falling in. This swirling disk of material\",\"stop_reason\":\"max_tokens\",\"stop\":null}'\n"
     ]
    }
   ],
   "source": [
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Here is a simple explanation of black holes for 8th graders:\n",
      "\n",
      "- A black hole is a region in space where gravity is so strong that nothing can escape from it, even light. This region is called the \"event horizon.\"\n",
      "\n",
      "- Black holes form when very massive stars collapse. When the star runs out of nuclear fuel, gravity makes it collapse. The matter gets compressed into a very small space, creating a black hole. \n",
      "\n",
      "- Black holes are called \"black\" because they do not let any light escape. The gravity is so intense that it swallows everything, even light waves. So we can't see black holes directly. We can only detect them from their effects on nearby stars and gas.\n",
      "\n",
      "- Black holes have such strong gravity because all their mass is concentrated in a very small space. This creates a very steep gravitational slope near the event horizon that traps everything. \n",
      "\n",
      "- Not even light can escape once it crosses the event horizon. That's why it's called the \"point of no return.\" Anything that goes beyond the event horizon is lost forever to the black hole.\n",
      "\n",
      "- Black holes do not \"suck\" matter in. They simply have gravity like everything else. But their gravity is so extreme that nothing can escape once it gets close.\n",
      "\n",
      "- We can detect black holes from the X-rays and other radiation given off by material as it swirls around the black hole before falling in. This swirling disk of material\n"
     ]
    }
   ],
   "source": [
    "#response_body = json.loads(response.get('body').read())\n",
    "response_body = json.loads(temp)\n",
    "\n",
    "# text\n",
    "print(response_body.get('completion'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<botocore.response.StreamingBody at 0x1148c7070>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "response_body = response.get('body').read()\n",
    "\n",
    "# text\n",
    "response_body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'bytes' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[29], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m response_body \u001B[38;5;241m=\u001B[39m response\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbody\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mread()\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mresponse_body\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcompletion\u001B[39m\u001B[38;5;124m'\u001B[39m))\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'bytes' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "response_body = response.get('body').read()\n",
    "print(response_body.get('completion'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response_body.get(\"completions\")[0].get(\"data\").get(\"text\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body = json.dumps({\n",
    "    \"prompt\": \"\\n\\nHuman: explain black holes to 8th graders\\n\\nAssistant:\",\n",
    "    \"max_tokens_to_sample\": 300,\n",
    "    \"temperature\": 0.0,\n",
    "    \"top_p\": 0.9,\n",
    "})\n",
    "\n",
    "#modelId = 'anthropic.claude-v2'\n",
    "modelId = 'ai21.j2-mid-v1'\n",
    "accept = 'application/json'\n",
    "contentType = 'application/json'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = bedrock.invoke_model(body=body, modelId=modelId, accept=accept, contentType=contentType)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "response_body = json.loads(response.get('body').read())\n",
    "\n",
    "# text\n",
    "print(response_body.get('completion'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Inference Server (not currently working)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from IPython.display import Markdown, display \n",
    "\n",
    "#from dotenv import load_dotenv, find_dotenv\n",
    "#_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "#client = openai.OpenAI()\n",
    "#openai.api_base = \"http://localhost:1234/v1\" # point to the local server\n",
    "#openai.api_key = \"\" # no need for an API key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_response_to_prompt(prompt):\n",
    "    response = openai.ChatCompletion.create(model=\"gpt-4\", \\\n",
    "               messages=[{\"role\":\"user\",\"content\":prompt}], temperature=0)\n",
    "    return response.choices[0].message[\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "response = get_response_to_prompt(\"Who are you\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
